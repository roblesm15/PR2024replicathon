---
title: "Tutorial 1a: Exploring Pharmacological Data with the `rawPharmacoData` Dataset"
format: revealjs
smaller: true
echo: true
editor: 
  markdown: 
    wrap: 72
---

```{r echo=FALSE}
knitr::opts_chunk$set(cache=FALSE)
```

## Introduction

```{css, echo = FALSE}
code.sourceCode {
  font-size: 1.3em;
  /* or try font-size: xx-large; */
}
```

Probably the most important step of analyzing datasets is to actually
understand the data. This process is crucial to know what kind of
questions we can answer with it.

This tutorial has code that will help guiding you through this process
with the `rawPharmacoData` dataset.

Make sure you understand the experimental design of the two studies well
and try to link each variable to this experimental design. Also, make
sure you understand what each *R* command is doing. Feel free to hack
the code!

When it makes sense, we include examples for answering the question
using both base R and the tidyverse packages. There's usually more than
one way of doing things in R!

If you have any question about the code, ask one of the mentors. Also
remember that [Google](https://www.google.com) search and
[ChatGPT](https://chatgpt.com) can aid you in data science tasks.

## Setup Workspace

We start by loading the tidyverse family of packages.

```{r}
library(tidyverse)
```

There are [several pre-defined themes for plotting with
`ggplot2`](https://ggplot2.tidyverse.org/reference/ggtheme.html). While
the default "`theme_gray`" is nice, we will set the default to
"`theme_bw`" using the `theme_set` function.

```{r}
theme_set(theme_bw())
```

## Load Raw Dataset

Let's start by loading the `RDS` file containing the raw pharmacological
data.

```{r read-rds}
pharmacoData <- readRDS(file.path("..", "data", "rawPharmacoData.rds"))
```

## Exploratory Analysis

We can take a quick peek at the data using the `head` and `str`
functions.

-   What kind of variables are in the data?

-   Are these variables numerical and/or categorical?

-   What does each column represent?

```{r peek-data}
head(pharmacoData)
str(pharmacoData)
```

------------------------------------------------------------------------

Next, we can count the number of drugs and cell lines in the dataset.

```{r count-uniq}
## using base R
length(unique(pharmacoData$cellLine))
length(unique(pharmacoData$drug))

## with the tidyverse
pharmacoData |>
    summarize(nCellLines = n_distinct(cellLine),
              nDrugs     = n_distinct(drug))
```

------------------------------------------------------------------------

Let's also try something a little more complex. We can also count the
number of unique drug concentrations **in each study** separately.

```{r count-conc}
## with base R
tapply(pharmacoData$concentration, pharmacoData$study,
       function(x) { length(unique(x)) })

## with the tidyverse
pharmacoData |>
    group_by(study) |>
    summarize(n = n_distinct(concentration))
```

------------------------------------------------------------------------

One of the first things data scientists do when digging into new data is
to explore their distributions. Histograms visualize the data
distributions and can also point us towards statistical models to use.
The code below transforms the concentration values to the logarithmic
scale and plots a histogram separately for each study.

```{r conc-hist, fig.width = 8, fig.height = 4, warning = FALSE}
pharmacoData |>
    ggplot(aes(x = log2(concentration))) +
    geom_histogram(fill = "gray", color = "black", binwidth = 1) +
    facet_wrap(~ study) +
    ggtitle("Distributions of concentrations by study")
```

Based on these plots, which study would you say has the most consistent
experimental protocol?

> Place your answer here

------------------------------------------------------------------------

Viability scores are the percentage of cells that survive upon exposure
to a certain drug. Below, we will explore the range of the data and
calculate how many data points are below 0 and above 100.

```{r viability-summaries}
## with base R
range(pharmacoData$viability)
sum(pharmacoData$viability < 0)
sum(pharmacoData$viability > 100)

## with the tidyverse
pharmacoData |>
    summarize(min_viability = min(viability),
              max_viability = max(viability),
              n_too_small   = sum(viability < 0),
              n_too_big     = sum(viability > 100))
```

------------------------------------------------------------------------

We can also compare the distribution of viability scores between the two
studies using density plots.

```{r log-density, fig.width = 8, fig.height = 4, warning = FALSE}
pharmacoData |>
    ggplot(aes(x = viability, group = study, fill = study, color = study)) +
    geom_density(alpha = 1/4) +
    xlim(0, 170) +
    ggtitle("Distributions of viability scores by study")
```

Based on the distribution of the viability scores, would you say there
are obvious differences between the two studies?

> Place your answer here

------------------------------------------------------------------------

The code below plots the viability scores as box-plots for each drug,
stratified by the two studies. We highlight the region of the plot where
viability scores should fall (between 0 and 100).

```{r viability-boxplots, fig.width = 8, fig.height = 4, warning = FALSE}
gp <- pharmacoData |>
    ggplot(aes(y = viability, x = drug, fill = study)) +
    scale_x_discrete() + 
    annotate(geom = "rect", ymin = 0, ymax = 100, xmin = -Inf, xmax = Inf,
             fill = 'black', alpha = 1/6) +
    geom_boxplot(outlier.alpha = 1/5) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 1/2)) +
    ggtitle("Distributions of viability scores by drug and study")
gp
```

There appear to be a few outliers with incredibly high viability scores!

------------------------------------------------------------------------

We should keep this in mind, but to get a better look at the majority of
the data, we can limit the y-axis of the plot.

```{r viability-boxplots-limit, fig.width = 8, fig.height = 4, warning = FALSE}
gp + ylim(0, 200)
```

Can you tell something about the toxic properties of the different
drugs? Are these properties consistent across studies?

> Place your answer here

## Confirmatory Analysis

So far, we have visually inspected plots of the data to answer scientific questions. This is typically referred to as "*Exploratory* Data Analysis" (EDA).  

-   This type of analysis is useful for getting a sense of what the data looks like and getting a personal sense of what scientific questions might be worth further investigation.
-   However, visual inspection is imprecise; what looks like a large difference to one person might be small to another

*Confirmatory analysis* allows to quantify whether the differences we might see in a plot are actually "significant" (whether they actually might mean something) or whether they really just result from the randomness of experimentation.

------------------------------------------------------------------------

A **statistical hypothesis test** is a procedure to tell us whether our results might be "statistically significant" -- meaning, not just due to random experimental error alone. 

- To perform a hypothesis test, we first formulate a *null hypothesis*: a condition under which we consider absolutely no effect to have occurred.
- For example, if we want to know whether the two studies in this data differed in terms of viability across drugs and cell lines, we might compare the *mean viability* score. 
- In this case, our null hypothesis is that 

$$\text{mean viability in CCLE} - \text{mean viability in GDSC} = 0$$  

- To test this hypothesis statistically, we choose a test that compares means. The most common is the *t-test*.
- There are other types of hypotheses and ways of testing them too; we won't go into the mathematical details here, but **you can read more about such procedures in the Supplementary Tutorial, Supplement: Statistical Hypothesis Testing**

------------------------------------------------------------------------

We can test whether the difference between the mean viability across studies is "statistically significant" using the `t.test` function below:

```{r}
t.test(viability ~ study, data = pharmacoData)
```


The **p-value** tells us the *probability* of sampling data with a difference in means as large as our own sample had the "true" difference been equal to 0. If the p-value is below some low threshold -- commonly 0.05 or 0.01 -- then we can say we've "rejected the null hypothesis", meaning that the difference in means is probably *not* just due to randomness.  

The p-value of the above is 2.2e-16. Does this indicate that the difference in mean viability between CCLE and GDSC is more than just random experimental error?  

> Place your answer here

---

